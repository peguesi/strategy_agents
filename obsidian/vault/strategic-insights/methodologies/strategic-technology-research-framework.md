# Strategic Technology Research & Selection Framework

## Overview

Universal methodology for researching, selecting, and implementing technology solutions for organizations. Designed to prevent common research failures through systematic validation of assumptions and clear decision criteria.

## Core Principles

1. **Build on Existing Intel** - Leverage pre-engagement research and client interactions
2. **Scale Discovery Effort** - Target validation based on confidence levels, not comprehensive rediscovery
3. **Question Critical Assumptions** - Validate high-impact unknowns before solution commitment
4. **Match Solution to Reality** - Align recommendations with validated organizational context
5. **Document Decision Logic** - Make trade-offs and priorities explicit and traceable
6. **Validate Through Implementation** - Framework evolves based on real-world outcomes

---

## Phase 0: Intel Processing & Framework Application

### Step 1: Intel Inventory & Confidence Assessment

#### Systematic Intel Processing
Process all existing client intelligence against framework structure:

**Sources to Process:**
- Pre-sales meeting transcripts and summaries
- Chat logs and informal communications  
- Signed proposals and work orders
- Previous research or documentation
- Stakeholder interviews and notes

**Intel Confidence Classification:**
```markdown
For each framework section, assess existing intel quality:

□ **High Confidence** - Validated through multiple sources/interactions, recent data
□ **Medium Confidence** - Single source or somewhat dated information
□ **Low Confidence** - Assumptions based on limited data or inference
□ **Unknown** - Clear gaps requiring discovery
□ **Critical Unknown** - Gaps that could fundamentally change solution approach
```

#### Framework Intel Mapping Template
```markdown
## Problem Space Assessment
- Problem definition: [Confidence Level] - [Intel Source]
- Scope boundaries: [Confidence Level] - [Intel Source]  
- Success criteria: [Confidence Level] - [Intel Source]
- User actor mapping: [Confidence Level] - [Intel Source]

## Scale Reality Assessment  
- Current user count: [Confidence Level] - [Intel Source]
- Volume/transaction data: [Confidence Level] - [Intel Source]
- Growth projections: [Confidence Level] - [Intel Source]
- Economic constraints: [Confidence Level] - [Intel Source]

## Organizational Context Assessment
- Technical capability: [Confidence Level] - [Intel Source]
- Budget parameters: [Confidence Level] - [Intel Source]  
- Change capacity: [Confidence Level] - [Intel Source]
- Priority matrix: [Confidence Level] - [Intel Source]

## Integration & Technical Requirements
- Existing systems: [Confidence Level] - [Intel Source]
- Required integrations: [Confidence Level] - [Intel Source]
- Technical constraints: [Confidence Level] - [Intel Source]
- Compliance requirements: [Confidence Level] - [Intel Source]
```

### Step 2: Critical Assumption Identification

#### High-Impact Unknown Analysis
Identify assumptions that could fundamentally change solution approach:

**Decision-Critical Unknowns:**
```markdown
□ Scale assumptions that affect economic viability
□ Technical capability assumptions that affect solution complexity
□ Integration assumptions that affect feasibility  
□ Budget assumptions that affect solution tier/approach
□ Timeline assumptions that affect implementation approach
□ User adoption assumptions that affect rollout strategy
```

**Assumption Risk Assessment:**
```markdown
For each Critical Unknown:
- **Assumption Statement:** [What are we assuming?]
- **Impact if Wrong:** [How would this change our approach?]
- **Current Confidence:** [1-10 scale]
- **Validation Priority:** [High/Medium/Low based on decision impact]
- **Validation Method:** [How will we test this?]
```

---

## Phase 1: Conditional Discovery Planning

### Step 3: Discovery Scope Decision Framework

#### Confidence-Based Discovery Targeting

**Discovery Decision Tree:**
```markdown
**High Confidence Areas** (Skip Discovery)
→ Proceed directly to solution research
→ Monitor during implementation for variance

**Medium Confidence Areas** (Light Validation)  
→ Confirm during other discovery activities
→ Include validation questions in scheduled sessions
→ Document assumptions for implementation monitoring

**Low Confidence Areas** (Targeted Discovery)
→ Dedicated discovery sessions focused on these areas
→ Structured validation methodology
→ Update framework assessment based on findings

**Critical Unknowns** (Deep Discovery Priority)
→ Must resolve before any solution decisions
→ Comprehensive validation methodology
→ Go/No-Go decision criteria based on findings
```

#### Scaled Discovery Methodology

**Minimal Discovery (Mostly High Confidence Intel):**
```markdown
Focus Areas:
- Validation of 1-2 Critical Unknowns only
- Implementation logistics confirmation
- Stakeholder alignment verification

Methods:
- Single discovery session (2-3 hours)
- Targeted stakeholder interviews
- Quick technical validation calls

Timeline: 1-2 weeks maximum
```

**Moderate Discovery (Mixed Confidence Levels):**
```markdown
Focus Areas:  
- Critical Unknown resolution
- Low Confidence area validation
- Implementation readiness assessment

Methods:
- 2-3 focused discovery sessions
- Workflow observation for key processes
- Technical integration proof-of-concept

Timeline: 2-4 weeks
```

**Comprehensive Discovery (Many Critical Unknowns):**
```markdown
Focus Areas:
- Systematic validation across all framework areas
- Detailed workflow analysis and documentation
- Complete technical and organizational assessment

Methods:
- Full discovery methodology (quantitative + qualitative + technical)
- Multiple stakeholder interview rounds
- Comprehensive integration testing

Timeline: 4-8 weeks
Note: Consider if engagement is properly scoped if comprehensive discovery needed
```

### Step 4: Targeted Discovery Execution

#### Discovery Session Planning Based on Confidence Assessment

**Session Structure Template:**
```markdown
## Discovery Session: [Focus Area]
**Objective:** Validate [specific assumptions] and resolve [critical unknowns]
**Confidence Baseline:** [What we think we know]
**Validation Targets:** [What we need to confirm/discover]
**Decision Impact:** [How this affects solution approach]

**Session Agenda:**
1. Assumption validation questions
2. Gap-filling information gathering  
3. Implementation factor confirmation
4. Risk factor identification

**Success Criteria:**
- [Specific assumptions confirmed/refuted]
- [Critical unknowns resolved to acceptable confidence level]
- [Implementation factors validated for planning]
```

#### Post-Discovery Framework Update

**Intel Confidence Reassessment:**
```markdown
After each discovery activity:
□ Update confidence levels for validated areas
□ Document new information and sources
□ Identify any new Critical Unknowns discovered
□ Adjust solution research scope based on validated requirements
□ Update risk assessment based on discovery findings
```

---

## Phase 2: Solution Research & Decision

### Step 5: Intel-Informed Solution Research

#### Solution Space Mapping (Confidence-Weighted)

**Category Definition Based on Validated Requirements:**
```markdown
## Solution Categories  
Primary Category: [Based on validated problem definition]
Adjacent Categories: [Alternative approaches given confirmed constraints]
Ruled-Out Approaches: [Eliminated by discovery findings]

## Integration Ecosystem (Confidence-Assessed)
Core System Requirements: [High confidence requirements]
Required Integrations: [Validated as critical]  
Desired Integrations: [Medium confidence/nice-to-have]
Future Integrations: [Low confidence/roadmap items]
```

#### Confidence-Weighted Complexity Assessment
```markdown
## Solution Complexity Levels
□ Configuration-Based (work within tool paradigm)
□ Low-Code Platform (build with constraints)  
□ Custom Development (build anything)
□ Hybrid Approach (multiple tools/methods)

**Complexity Confidence Factors:**
- Technical capability assessment confidence: [Level]
- Integration requirement confidence: [Level]
- Resource availability confidence: [Level]
- Timeline constraint confidence: [Level]
```

### Step 6: Option Evaluation with Confidence Weighting

#### Research Template Per Option (Confidence-Adjusted)
```markdown
# Option: [Solution Name]

## Fit Assessment (Confidence-Weighted)
- Solves primary problem: [Y/N] (Confidence: [Level]) [Intel Source]
- Handles current scale: [Y/N] (Confidence: [Level]) [Intel Source]
- Matches technical capability: [Y/N] (Confidence: [Level]) [Intel Source]  
- Within budget parameters: [Y/N] (Confidence: [Level]) [Intel Source]

## Risk Assessment by Confidence Level
**High Confidence Factors:**
- [Factors we're certain about and their implications]

**Medium/Low Confidence Factors:**  
- [Factors requiring assumption/monitoring during implementation]

**Critical Unknowns:**
- [Remaining unknowns that affect this option's viability]
```

---

## Phase 3: Decision & Implementation

### Step 7: Confidence-Weighted Decision Framework

#### Decision Matrix with Confidence Factors
```markdown
## Criteria Weighting (Adjusted for Confidence)
- Solves core problem: [X]% weight (Confidence: [Level])
- Fits current scale: [X]% weight (Confidence: [Level])
- Within budget: [X]% weight (Confidence: [Level])
- Implementation feasibility: [X]% weight (Confidence: [Level])
- Growth potential: [X]% weight (Confidence: [Level])

## Confidence-Adjusted Scoring
Weight high-confidence factors more heavily in final decision
Flag low-confidence factors as implementation monitoring priorities
```

#### Risk Assessment Based on Intel Confidence
```markdown
## Implementation Risks by Confidence Level

**High-Risk Factors (Low Confidence Intel):**
- [Specific risks from unvalidated assumptions]
- [Mitigation plans and monitoring strategies]

**Medium-Risk Factors (Medium Confidence Intel):**
- [Areas requiring early implementation validation]
- [Contingency plans if assumptions prove incorrect]

**Low-Risk Factors (High Confidence Intel):**
- [Areas for standard implementation approach]
```

### Step 8: Implementation Planning with Confidence Monitoring

#### Phased Implementation Based on Intel Confidence
```markdown
## Phase 1: High-Confidence Foundation
- Scope: [Components based on high-confidence intel]
- Success criteria: [Well-understood objectives]
- Resources required: [Validated requirements]

## Phase 2: Medium-Confidence Expansion  
- Scope: [Components requiring assumption validation]
- Success criteria: [Includes assumption testing]
- Adjustment triggers: [When to modify approach]

## Phase 3: Low-Confidence/Unknown Area Management
- Scope: [Components with highest uncertainty]
- Discovery-in-implementation: [Learning approach]
- Pivot planning: [Alternative approaches ready]
```

#### Intel Confidence Monitoring During Implementation
```markdown
**Assumption Tracking:**
- Monitor assumption accuracy during implementation
- Document variance between expected vs actual outcomes
- Capture lessons learned for framework improvement
- Update confidence assessment methodology based on results

**Framework Evolution:**
- Record which intel sources prove most/least reliable
- Identify intel gathering methods that improve accuracy
- Develop improved confidence assessment criteria
- Enhance assumption risk assessment based on real outcomes
```

---

## Framework Validation & Evolution

### Decision Audit Questions (Confidence-Aware)
Before finalizing any recommendation:

1. **Intel Quality Validation**
   - Are high-confidence assessments based on multiple, recent sources?
   - Have Critical Unknowns been resolved to acceptable confidence levels?
   - Are assumptions clearly flagged with confidence levels and validation plans?
   - Are decision criteria weighted appropriately for intel confidence?

2. **Solution Alignment (Risk-Adjusted)**  
   - Does complexity match validated (not assumed) organizational capability?
   - Does cost structure align with confirmed (not projected) value delivery timeline?
   - Are integration requirements based on validated (not documented) system capabilities?
   - Is implementation approach scaled to intel confidence levels?

3. **Risk Management (Confidence-Based)**
   - What happens if low-confidence assumptions prove incorrect?
   - Are monitoring and adjustment mechanisms in place for uncertain factors?
   - Are alternative approaches ready for critical assumption failures?
   - Is client expectation set appropriately for intel confidence levels?

### Post-Implementation Learning Capture

#### Intel Accuracy Tracking
```markdown  
## Implementation Outcome vs Intel Assessment
- Intel source accuracy: [Which sources proved most reliable?]
- Confidence level accuracy: [Were confidence assessments realistic?]
- Assumption accuracy: [Which assumptions proved correct/incorrect?]
- Discovery effectiveness: [Did targeted discovery resolve Critical Unknowns?]

## Framework Improvement Insights
- Intel gathering method effectiveness
- Confidence assessment criteria refinement
- Discovery targeting methodology improvements
- Risk prediction accuracy enhancement
```

---

## Quick Reference Guide

### Intel Processing Checklist
- [ ] All pre-engagement sources processed against framework structure
- [ ] Confidence levels assigned to each framework section
- [ ] Critical Unknowns identified and prioritized
- [ ] Discovery scope determined based on confidence assessment
- [ ] Assumption risk assessment completed for decision-critical factors

### Discovery Decision Criteria
- **Skip Discovery**: High confidence across all decision-critical factors
- **Light Discovery**: Medium confidence on most factors, few Critical Unknowns
- **Moderate Discovery**: Mixed confidence levels, several Critical Unknowns
- **Comprehensive Discovery**: Low confidence across multiple areas, many Critical Unknowns

### Confidence-Based Red Flags
- Making decisions based on low-confidence intel without validation
- Proceeding to implementation with unresolved Critical Unknowns
- Weighting low-confidence factors equally with high-confidence factors in decisions
- Failing to monitor assumption accuracy during implementation

---

**Framework Status**: Enhanced with Intel Processing & Conditional Discovery - June 24, 2025  
**Key Enhancement**: Confidence-based discovery targeting prevents over/under-discovery  
**Next Evolution**: Product development for systematic intel processing and framework application